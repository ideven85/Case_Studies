{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 6.390 Spring 2024 Homework 6\n",
    "\n",
    "**If you haven't already, please hit :**\n",
    "\n",
    "`File` -> `Save a Copy in Drive`\n",
    "\n",
    "**to copy this notebook to your Google drive, and work on a copy. If you don't do this, your changes won't be saved!**"
   ],
   "metadata": {
    "id": "USxWiWVJmnAk"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "asA46P9SBahm",
    "ExecuteTime": {
     "end_time": "2025-11-05T10:34:42.700617Z",
     "start_time": "2025-11-05T10:34:38.939544Z"
    }
   },
   "source": [
    "# Run this cell to download the test functions for HW 6\n",
    "\n",
    "\n",
    "import optim as our_optim\n",
    "from hw06_tests import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  hw06_tests.zip\r\n",
      "   creating: hw06_tests\r\n",
      "  inflating: hw06_tests/lib.py       \r\n",
      "  inflating: hw06_tests/plotting.py  \r\n",
      "  inflating: hw06_tests/hw06_tests.py  \r\n",
      "  inflating: hw06_tests/utils.py     \r\n",
      "  inflating: hw06_tests/test_suite.py  \r\n",
      "  inflating: hw06_tests/sequential_expected.py  \r\n",
      "   creating: hw06_tests/data\r\n",
      "  inflating: hw06_tests/optim.py     \r\n",
      "  inflating: hw06_tests/data/data5_train.csv  \r\n",
      "  inflating: hw06_tests/data/data3_train.csv  \r\n",
      "  inflating: hw06_tests/data/data4_train.csv  \r\n",
      "  inflating: hw06_tests/data/data4_validate.csv  \r\n",
      "  inflating: hw06_tests/data/data3_validate.csv  \r\n",
      "  inflating: hw06_tests/data/dataXor_train.csv  \r\n",
      "  inflating: hw06_tests/data/data2_train.csv  \r\n",
      "  inflating: hw06_tests/data/data2_validate.csv  \r\n",
      "  inflating: hw06_tests/data/data5_validate.csv  \r\n",
      "  inflating: hw06_tests/data/data3class_train.csv  \r\n",
      "  inflating: hw06_tests/data/data1_validate.csv  \r\n",
      "  inflating: hw06_tests/data/data1_train.csv  \r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remember: To look at the functions and test cases we've provided, after running the import cell above, click on the üìÅ icon on the sidebar and double-click the `hw06_tests.py` file to open it in a new window. Running a test case should hopefully straightforward: find the corresponding function (usually named `test_<name-of-func>` and pass your implemented function into it)."
   ],
   "metadata": {
    "id": "xDKhQRzMo4KZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5) Implementing Neural Networks"
   ],
   "metadata": {
    "id": "T2ffmsy7zS8u"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5.1)"
   ],
   "metadata": {
    "id": "uc5dOAttze7x"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Module:\n",
    "    def sgd_step(self, lrate):\n",
    "        pass  # For modules w/o weights\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, m, n):\n",
    "        # initializes the weights randomly and offsets as 0\n",
    "        self.m, self.n = (m, n)  # (in size, out size)\n",
    "        self.W0 = np.zeros([self.n, 1])  # (n x 1)\n",
    "        self.W = np.random.normal(0, 1.0 * m ** (-0.5), [m, n])  # (m x n)\n",
    "\n",
    "    def forward(self, A):\n",
    "        # store the input matrix for future use\n",
    "        self.A = A  # (m x b)  Hint: make sure you understand what b stands for\n",
    "        return None  # Your code (n x b)\n",
    "\n",
    "    def backward(self, dLdZ):\n",
    "        # dLdZ is (n x b), uses stored self.A\n",
    "        # store the derivatives for use in sgd_step and returd dLdA\n",
    "        self.dLdW = None  # Your code\n",
    "        self.dLdW0 = None  # Your code\n",
    "        return None  # Your code: return dLdA (m x b)\n",
    "\n",
    "    def sgd_step(self, lrate):  # Gradient descent step\n",
    "        self.W = None  # Your code\n",
    "        self.W0 = None  # Your code"
   ],
   "metadata": {
    "id": "aG6BQTQUzgO0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_linear(Linear)"
   ],
   "metadata": {
    "id": "Al-x4bLgzihb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "5.2.1) Tanh"
   ],
   "metadata": {
    "id": "dmTIKm8ozmQy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Tanh(Module):  # Layer activation\n",
    "    def forward(self, Z):\n",
    "        self.A = None  # Your code\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):  # Uses stored self.A\n",
    "        return None  # Your code: return dLdZ"
   ],
   "metadata": {
    "id": "-3jymUzUznDh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_tanh(Tanh)"
   ],
   "metadata": {
    "id": "4UkHNh4TzpKJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "5.2.2) ReLU"
   ],
   "metadata": {
    "id": "nmqtXiVRz2B5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class ReLU(Module):  # Layer activation\n",
    "    def forward(self, Z):\n",
    "        self.A = None  # Your code\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):  # uses stored self.A\n",
    "        return None  # Your code: return dLdZ"
   ],
   "metadata": {
    "id": "vAnJDPLjz5tC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_relu(ReLU)"
   ],
   "metadata": {
    "id": "d-3Pd3nMz8Oe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "5.2.3) Softmax"
   ],
   "metadata": {
    "id": "YEeGEN6L0AUP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class SoftMax(Module):  # Output activation\n",
    "    def forward(self, Z):\n",
    "        return None  # Your code\n",
    "\n",
    "    def backward(self, dLdZ):  # Assume that dLdZ is passed in\n",
    "        return dLdZ\n",
    "\n",
    "    def class_fun(self, Ypred):\n",
    "        # Returns the index of the most likely class for each point as vector of shape (b,)\n",
    "        return None  # Your code"
   ],
   "metadata": {
    "id": "yuIitjM40B7V"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_softmax(SoftMax)"
   ],
   "metadata": {
    "id": "Qqo00e5O0DjO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "5.3) Loss Module - NLLM"
   ],
   "metadata": {
    "id": "JHTH2Ci_0K_A"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class NLL(Module):  # Loss\n",
    "    def forward(self, Ypred, Y):\n",
    "        # returns loss as a float\n",
    "        self.Ypred = Ypred\n",
    "        self.Y = Y\n",
    "        return None  # Your code\n",
    "\n",
    "    def backward(self):  # Use stored self.Ypred, self.Y\n",
    "        # note, this is the derivative of loss with respect to the input of softmax\n",
    "        return None  # Your code"
   ],
   "metadata": {
    "id": "I4MLW9GK0NNJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_nll(NLL)"
   ],
   "metadata": {
    "id": "uGX_Y5cd0PjI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "5.4) Sequential"
   ],
   "metadata": {
    "id": "B-uF9dkH0TnW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Sequential:\n",
    "    def __init__(self, modules, loss):  # List of modules, loss module\n",
    "        self.modules = modules\n",
    "        self.loss = loss\n",
    "\n",
    "    def sgd(self, X, Y, iters=100, lrate=0.005):  # Train\n",
    "        D, N = X.shape\n",
    "        for it in range(iters):\n",
    "            pass  # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, Xt):  # Compute Ypred\n",
    "        for m in self.modules:\n",
    "            Xt = m.forward(Xt)\n",
    "        return Xt\n",
    "\n",
    "    def backward(self, delta):  # Update dLdW and dLdW0\n",
    "        # Note reversed list of modules\n",
    "        for m in self.modules[::-1]:\n",
    "            delta = m.backward(delta)\n",
    "\n",
    "    def sgd_step(self, lrate):  # Gradient descent step\n",
    "        for m in self.modules:\n",
    "            m.sgd_step(lrate)\n",
    "\n",
    "    def print_accuracy(self, it, X, Y, cur_loss, every=250):\n",
    "        # Utility method to print accuracy on full dataset, should\n",
    "        # improve over time when doing SGD. Also prints COURSE/questions/nn loss,\n",
    "        # which should decrease over time. Call this on each iteration\n",
    "        # of SGD!\n",
    "        if it % every == 1:\n",
    "            cf = self.modules[-1].class_fun\n",
    "            acc = np.mean(cf(self.forward(X)) == cf(Y))\n",
    "            print(\"Iteration =\", it, \"\tAcc =\", acc, \"\tLoss =\", cur_loss)"
   ],
   "metadata": {
    "id": "l7TaB7_h0YHY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_sequential(Sequential, Linear, Tanh, ReLU, SoftMax, NLL)"
   ],
   "metadata": {
    "id": "f66uXTx20a-9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6) Neural Network Packages\n",
    "\n",
    "**For problem 6, use the function `run_pytorch_2d` in `hw06_tests.py`**\n",
    "\n",
    "Here's an example to help you get started."
   ],
   "metadata": {
    "id": "skTmKM65tAa6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "layers = [\n",
    "    nn.Linear(in_features=2, out_features=2, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=2, out_features=2, bias=True),\n",
    "]\n",
    "_ = run_pytorch_2d(\n",
    "    \"2\", layers=layers, epochs=200, trials=1, verbose=False, display=True\n",
    ")"
   ],
   "metadata": {
    "id": "bCSLHEmq7qmZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, each layer is an element of the layers list. in_features is the number of units in the previous layer. out_features is the number of units in the current layer. We consider the first layer to be the hidden layer. So you should be able to just modify a parameter of the example we gave."
   ],
   "metadata": {
    "id": "RnCOHd1O6KPt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##6.5)"
   ],
   "metadata": {
    "id": "IbjbHKynpD0T"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "layers = [\n",
    "    nn.Linear(in_features=2, out_features=100, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=100, out_features=100, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=100, out_features=2, bias=True),\n",
    "]\n",
    "# By setting `return_history=True` argument in the `run_pytorch_2d` call, we can\n",
    "# have all the history values returned for inspection\n",
    "model, history = run_pytorch_2d(\n",
    "    \"2\",\n",
    "    layers=layers,\n",
    "    epochs=300,\n",
    "    trials=5,\n",
    "    verbose=False,\n",
    "    display=True,\n",
    "    return_history=True,\n",
    ")\n",
    "# the history variable will be a dictionary, with the following keys:\n",
    "# print(history.keys())\n",
    "# \"epoch_loss\", \"epoch_val_loss\", \"epoch_acc\", \"epoch_val_acc\""
   ],
   "metadata": {
    "id": "IRhhqOj1o_Wp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XItSk-mZK6J6"
   },
   "source": [
    "## 6.7\n",
    "\n",
    "You may find the cell below useful for starting your work for this question."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ruGoFiTOK5qU"
   },
   "source": [
    "points = np.array(\n",
    "    [[-1, 0], [1, 0], [0, -11], [0, 1], [-1, -1], [-1, 1], [1, 1], [1, -1]]\n",
    ")\n",
    "\n",
    "deterministic = True\n",
    "if deterministic:\n",
    "    torch.manual_seed(10)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(10)\n",
    "\n",
    "points = torch.Tensor(points)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
