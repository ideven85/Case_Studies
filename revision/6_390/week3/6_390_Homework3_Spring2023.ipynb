{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 6.390 Spring 2023 Homework 3\n",
    "\n",
    "**If you haven't already, please hit :**\n",
    "\n",
    "`File` -> `Save a Copy in Drive`\n",
    "\n",
    "**to copy this notebook to your Google drive, and work on a copy. If you don't do this, your changes won't be saved!**"
   ],
   "metadata": {
    "id": "USxWiWVJmnAk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asA46P9SBahm"
   },
   "outputs": [],
   "source": [
    "# Run this cell to download the test functions for HW 3\n",
    "!rm -f hw03_tests.py\n",
    "!wget --quiet --no-check-certificate https://introml.mit.edu/_static/spring23/homework/hw03/hw03_tests.py\n",
    "from hw03_tests import *\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymWFpDbDqsbR"
   },
   "source": [
    "#### 10.2 Finding the Best Parameters (Optional)\n",
    "\n",
    "Let's load the Boston Housing dataset. Our goal is to build a linear regression model (with regularization) to predict the TARGET_VAL (which is the median value of owner-occupied homes) using all other available features in the dataset.\n",
    "\n",
    "For more information about the Boston housing dataset, please visit this [link](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html).\n",
    "\n",
    "Note that the data pre-processing routine below normalizes each feature. You will learn more about Feature transformations in Week 5.\n",
    "\n",
    "In what follows, we use Cross-Validation to select the best hyperparameters for gradient descent on the ridge regression model. Using the best hyperparameters, we will then make predictions on a reserved test set. You will also compare the results when using the gradient descent based implementation vs the analytic (closed form) solution."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "## DO NOT EDIT BELOW.\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Pre-Processing the data and returning the train and test sets.\n",
    "\n",
    "# load the dataset and do some data exploration\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "\n",
    "raw_data = np.concatenate((data, target[:, None]), axis=1)\n",
    "xvars = [\n",
    "    \"CRIM\",\n",
    "    \"ZN\",\n",
    "    \"INDUS\",\n",
    "    \"CHAS\",\n",
    "    \"NOX\",\n",
    "    \"RM\",\n",
    "    \"AGE\",\n",
    "    \"DIS\",\n",
    "    \"RAD\",\n",
    "    \"TAX\",\n",
    "    \"PTRATIO\",\n",
    "    \"B\",\n",
    "    \"LSTAT\",\n",
    "]\n",
    "yvars = [\"TARGET_VAL\"]\n",
    "\n",
    "data = pd.DataFrame(raw_data, columns=xvars + yvars)\n",
    "\n",
    "# Get the train and test splits to be used later.\n",
    "X_train, y_train, X_test, y_test = get_data_splits_with_transforms(data, xvars, yvars)"
   ],
   "metadata": {
    "id": "3Zf7gYm-E_gk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55Z4plUevNrU"
   },
   "source": [
    "**CODE REQUIRED HERE** Before we start using the Boston Housing dataset, let's implement the `ridge_gd` function. Given an input `X_train`, `y_train`, `lam`, `theta`, `step_size_fn`, and `num_steps`, run gradient descent on `X_train` and `y_train` starting from `theta = (dx1) vector of zeros`. Return the value of $\\theta$ after running `num_steps` iterations of gradient descent.\n",
    "\n",
    "```\n",
    "inputs:\n",
    "  X_train: a dxn numpy array\n",
    "  y_train: a 1xn numpy array\n",
    "  lam: lambda\n",
    "  step_size_fn: a function that takes in i, the current training iteration, and returns the step size for iteration i\n",
    "  num_steps: number of iterations\n",
    "\n",
    "outputs:\n",
    "  theta: value of theta after num_steps iterations of gradient descent\n",
    "```\n",
    "**Hint**: Your implementations of `objective_func` and `objective_func_grad` are very useful here! \\\n",
    "**Hint**: You can also use your `gd` function \\\n",
    "**Hint**: Previously, you've minimized f as a function of x. Now, X and y are constant. What variable are you minimizing over now? \\"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def ridge_gd(X_train, y_train, lam, step_size_fn, num_steps=2000):\n",
    "    # TODO\n",
    "    # hint: start from theta = (dx1) vector of zeros"
   ],
   "metadata": {
    "id": "nf6OI1PCQoxf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "TestRidgeGD(ridge_gd)"
   ],
   "metadata": {
    "id": "7r3byrS1LRgP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcyFKWP233ir"
   },
   "source": [
    "**CODE REQUIRED HERE** In `cross_validate_gd`, run 5-fold cross-validation on the X and y dataset. Use gradient descent to train a linear model for the `X`, `y` data. We've provided a for loop that iterates over each split. In this code:\n",
    "\n",
    "```\n",
    "  X_train_split, y_train_split: data to use for training. This is a (d x n) numpy array, where n=the number of datapoints in k-1 folds\n",
    "  X_val_split, y_val_split: data to use for evaluating the model. This is a (d x n) numpy array, where n=the number of datapoints in 1 fold\n",
    "```\n",
    "\n",
    "**Hint**: Use `ridge_gd` here. \\\n",
    "**Hint**: Take a look at the solutions for last week's cross_validate code if you get stuck"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def cross_validate_gd(X, y, lam, step_size_fn):\n",
    "    \"\"\"\n",
    "    Returns k-fold cross-validation loss. On each of the k folds,\n",
    "    train a linear regression model using gradient descent. Return\n",
    "    the average loss across the k folds.\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    kf = KFold(n_splits=5)\n",
    "    for train_index, test_index in kf.split(X, y=y):\n",
    "        X_train_split, y_train_split = X[train_index].T, y[train_index].T\n",
    "        # TODO - train model on X_train_split, y_train_split using gradient descent\n",
    "        # hint - use variables step_size_fn and lam\n",
    "        # TODO - evaluate model on X_val_split, y_val_split, add loss to total_loss\n",
    "    return total_loss / kf.n_splits"
   ],
   "metadata": {
    "id": "jx4A_489Qk2p"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMkgs4rfXWMz"
   },
   "source": [
    "Now it's time to run grid search! We are interested in running grid search over $\\lambda \\in \\{{1e-4, 1e-3, \\cdots, 1e-1\\}}$ and $\\eta \\in \\{{1e-6, 1e-5, \\cdots, 1e-2\\}}$.\n",
    "\n",
    "These two cells are ready to run if you've correctly implemented `cross_validate_gd`. Use the outputs of these cells to answer the rest of problem 5.2.\n",
    "\n",
    "We've also already implemented `cross_validate_analytic` for you. This function returns the cross-validation loss for linear regression models trained with the analytic solution for the squared loss equation.\n",
    "\n",
    "**Note: The next two cells print the cross-validation loss, not the testing set loss! Run the last cell in this notebook for the testing set loss.**\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nttJDNnpc18o"
   },
   "source": [
    "learning_rates = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "lams = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "\n",
    "# This code runs grid search over the training parameters in `learning_rates` and `lams`\n",
    "for rate in learning_rates:\n",
    "    for lam in lams:\n",
    "        learning_rate_fn = lambda i: rate  # learning rate = `rate` throughout training\n",
    "        cross_validation_loss = cross_validate_gd(\n",
    "            X_train, y_train, lam, learning_rate_fn\n",
    "        )\n",
    "        print(\n",
    "            f\"Loss on dataset with lambda={lam}, rate={rate} : cross_validation_loss {cross_validation_loss:.6f}\"\n",
    "        )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bDvZz6A4NDBF"
   },
   "source": [
    "lams = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "\n",
    "# This code runs grid search over the training parameters in `lams`\n",
    "for lam in lams:\n",
    "    cross_validation_loss = cross_validate_analytic(X_train, y_train, lam).item()\n",
    "    print(\n",
    "        f\"Loss on dataset with lambda={lam}: cross_validation_loss {cross_validation_loss:.6f}\"\n",
    "    )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RKWyzPRkQXl"
   },
   "source": [
    "We will now use the best params found above to build a model on the entire training set (X_train, y_train), get the $\\theta$ values and use them to make predictions for the test set (X_test) and evaluate the error using the *actual* values (y_test). We will compare this error for the gradient descent based implementation vs the analytic solution.\n",
    "\n",
    "\n",
    "**CODE REQUIRED HERE**:\n",
    "\n",
    "1. Update **best_lam_gd** and **best_rate_gd** using the best $\\lambda$ and $\\eta$ values you found using **cross_validate_gd**() above.\n",
    "\n",
    "2. Update **best_lam_analytic** using the best $\\lambda$ value found by using **cross_validate_analytic**() above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "## DO NOT EDIT THESE FUNCTIONS.\n",
    "# Returns the test set predictions and error for the GD based implementation.\n",
    "def get_gd_predictions_and_error(\n",
    "    objective_func,\n",
    "    objective_func_grad,\n",
    "    gd_func,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    best_lam_gd,\n",
    "    best_rate_gd,\n",
    "):\n",
    "    num_steps = 5000\n",
    "    theta_gd = ridge_gd(\n",
    "        X_train.T, y_train.T, best_lam_gd, lambda i: best_rate_gd, num_steps=num_steps\n",
    "    )\n",
    "    gd_predictions = theta_gd.T @ X_test.T\n",
    "    gd_error = np.mean((gd_predictions - y_test) ** 2)\n",
    "    return gd_predictions, gd_error\n",
    "\n",
    "\n",
    "# Returns the test set predictions and error using the Analytic expression.\n",
    "def get_analytic_predictions_and_error(X_train, y_train, X_test, y_test, best_lam):\n",
    "    theta_analytic = ridge_analytic(X_train.T, y_train.T, best_lam)\n",
    "    analytic_predictions = theta_analytic.T @ X_test.T\n",
    "    analytic_error = np.mean((analytic_predictions - y_test) ** 2)\n",
    "    return analytic_predictions, analytic_error"
   ],
   "metadata": {
    "id": "wBfbRlxRMqUz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qhJFMnVUjm-Z"
   },
   "source": [
    "#### Using the functions above along with the best performing hyperparams to\n",
    "### determine the test set errors. Please specify the best lambda and learning\n",
    "### rates for the GD and Analytic cases that you found above.\n",
    "\n",
    "# GD\n",
    "best_lam_gd = None  ### TODO: to be specified\n",
    "best_rate_gd = None  ### TODO: to be specified\n",
    "\n",
    "# get_gd_predictions() function is defined in the hw03 code you imported at\n",
    "# the very top. Check the code out if you are curious about the implementation.\n",
    "gd_predictions, gd_error = get_gd_predictions_and_error(\n",
    "    objective_func,\n",
    "    objective_func_grad,\n",
    "    gd,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    best_lam_gd,\n",
    "    best_rate_gd,\n",
    ")\n",
    "\n",
    "# Analytic\n",
    "best_lam_analytic = None  ### TODO: to be specified\n",
    "\n",
    "# get_analytic_predictions_and_error() function is defined in the hw03 code\n",
    "# you imported at the very top. Check the code out if you are curious about the\n",
    "# implementation.\n",
    "analytic_predictions, analytic_error = get_analytic_predictions_and_error(\n",
    "    X_train, y_train, X_test, y_test, best_lam_analytic\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Test loss for GD based implementation={gd_error:0.3f}\")\n",
    "print(f\"Test loss for Analytic (closed form) implementation={analytic_error:0.3f}\")\n",
    "\n",
    "\n",
    "#### (Optional) Compare the results by viewing the scatter plots for predictions.\n",
    "plt.scatter(y_test, gd_predictions, color=\"red\", label=\"GD\")\n",
    "plt.scatter(y_test, analytic_predictions, color=\"blue\", label=\"Analytic\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Predictions vs Actual Scatter Plot\")\n",
    "plt.xlim([-3, 3])\n",
    "plt.ylim([-3, 3])\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}