{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 6.390 Spring 2023 Homework 4\n",
    "\n",
    "**If you haven't already, please hit :**\n",
    "\n",
    "`File` -> `Save a Copy in Drive`\n",
    "\n",
    "**to copy this notebook to your Google drive, and work on a copy. If you don't do this, your changes won't be saved!**"
   ],
   "metadata": {
    "id": "USxWiWVJmnAk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remember to copy over your implementation of gradient descent `gd()` from homework 3."
   ],
   "metadata": {
    "id": "-JsQq4yGz3nq"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "asA46P9SBahm",
    "ExecuteTime": {
     "end_time": "2025-11-05T10:38:01.547321Z",
     "start_time": "2025-11-05T10:37:59.753598Z"
    }
   },
   "source": [
    "# Run this cell to download the test functions for HW 4\n",
    "\n",
    "!wget --quiet --no-check-certificate https://introml.mit.edu/_static/spring23/homework/hw04/hw04_tests.py\n",
    "from hw04_tests import *\n",
    "\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our goal in this section will be to derive and implement appropriate gradient calculations that we can use with gd for optimization of the LLC objective. In the derivations below, we'll consider linear binary classifiers with offset; i.e., our collection of parameters is $\\theta$, $\\theta_0$.\n",
    "\n",
    "Recall that NLL loss for binary classification is defined as:\n",
    "\n",
    "$$L_{nll}(g, y) = -(y\\log g + (1-y)\\log (1-g))$$\n",
    "\n",
    "The objective function for linear logistic classification (LLC), a.k.a. logistic regression (LR), takes the mean of the NLL loss over all points and introduces a regularization term to this equation to make sure that the magnitude of $\\theta$ stays small.\n",
    "\n",
    "$$J_{lr}(\\theta, \\theta_0) = \\left [\\frac{1}{n}\\sum_{i=1}^n L_{nll}(\\sigma(\\theta\\cdot x^{(i)} + \\theta_0), y^{(i)})\\right ] + \\lambda ||\\theta||^2$$\n",
    "\n",
    "We're interested in applying our gradient descent procedure to this function in order to find the 'best' separator for our data, where 'best' is measured by the lowest possible LLC objective.\n",
    "\n",
    "For this question, you may find Section 4.4 of the notes particularly helpful."
   ],
   "metadata": {
    "id": "6Q0-sQn-ftAv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def gd(f, df, x0, step_size_fn, num_steps):\n",
    "    pass  # -------- COPY YOUR GD IMPLEMENTATION FROM HW3 --------"
   ],
   "metadata": {
    "id": "MKzfgbjgffVd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.1) Calculating the LLC objective\n",
    "First, implement the sigmoid function and implement NLL loss over the data points and separator. Using the latter function, implement the LLC objective. Note that these functions should work for matrix/vector arguments, so that we can compute the objective for a whole dataset with one call.\n",
    "\n",
    "Note that we're going to let X represent the full set of features across all the data points and y represent the full set of labels across all the data points. So `X` is $d\\times n$, `y` is $1\\times n$, `th` is $d\\times 1$, `th0` is $1\\times 1$, `lam` is a scalar.\n",
    "\n",
    "__Hint__: Look at `np.exp`, `np.log`\n",
    "\n",
    "In the test cases for this problem, we'll use the following `super_simple_separable` test dataset and `sep_e_separator` test separator.\n",
    "\n",
    "```\n",
    "def super_simple_separable():\n",
    "    X = np.array([[2, 3, 9, 12],\n",
    "                  [5, 2, 6, 5]])\n",
    "    y = np.array([[1, 0, 1, 0]])\n",
    "    return X, y\n",
    "\n",
    "sep_e_separator = np.array([[-0.40338351], [1.1849563]]), np.array([[-2.26910091]])\n",
    "\n",
    "\n",
    "__Note:__ In this section, you will code many individual functions, each of which depends on previous ones. We __strongly__ recommend that you test each of the components on your own to debug.\n",
    "\n",
    "Please use `np.sum` to take the sum of a matrix if needed."
   ],
   "metadata": {
    "id": "qizasSkWfyxr"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:42:02.283613Z",
     "start_time": "2025-11-05T10:42:02.277284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def super_simple_separable():\n",
    "    X = np.array([[2, 3, 9, 12], [5, 2, 6, 5]])\n",
    "    y = np.array([[1, 0, 1, 0]])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "sep_e_separator = np.array([[-0.40338351], [1.1849563]]), np.array([[-2.26910091]])"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:46:52.941895Z",
     "start_time": "2025-11-05T10:46:52.939764Z"
    }
   },
   "cell_type": "code",
   "source": "sep_e_separator[1]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.26910091]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "# returns a vector of the same shape as z\n",
    "def sigmoid(z):\n",
    "    return sep_e_separator[0].T * z + sep_e_separator[1]\n",
    "\n",
    "\n",
    "# X is dxn, y is 1xn, th is dx1, th0 is 1x1\n",
    "\n",
    "# returns a (1,n) array for the nll loss for each data point given th and th0\n",
    "\n",
    "\n",
    "def nll_loss(X, y, th, th0):\n",
    "    pass\n",
    "\n",
    "\n",
    "# X is dxn, y is 1xn, th is dx1, th0 is 1x1, lam is a scalar\n",
    "\n",
    "# returns a scalar for the llc objective over the dataset\n",
    "\n",
    "\n",
    "def llc_obj(X, y, th, th0, lam):\n",
    "    pass"
   ],
   "metadata": {
    "id": "h3P_uogxfzhn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Test case 1\n",
    "x_1, y_1 = super_simple_separable()\n",
    "th1, th1_0 = np.array([[-0.40338351], [1.1849563]]), np.array([[-2.26910091]])\n",
    "ans = llc_obj(x_1, y_1, th1, th1_0, 0.1)\n",
    "\n",
    "print(\"LLC objective over the test dataset with lambda 0.1\")\n",
    "print(\"Expected: 0.37394169100056684, your answer\", ans)\n",
    "# Test case 2\n",
    "ans2 = llc_obj(x_1, y_1, th1, th1_0, 0)\n",
    "\n",
    "print(\"LLC objective over the test dataset with lambda 0\")\n",
    "print(\"Expected: 0.21725772209560584, your answer\", ans2)"
   ],
   "metadata": {
    "id": "ZeU-BeRgf3nm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.2) Calculating the gradients\n",
    "\n",
    "Define a function `llc_obj_grad` that returns the gradient of the LLC objective function with respect to $\\theta$ and $\\theta_0$ in a single column vector. The last component of the gradient vector should be the partial derivative with respect to $\\theta_0$. Look at `np.vstack` as a simple way of stacking two matrices/vectors vertically. We have broken it down into pieces that mimic steps in the chain rule; this leads to code that is a bit inefficient but easier to write and debug. We can worry about efficiency later.\n",
    "\n",
    "__Note:__ In this section, you will code many individual functions, each of which depends on previous ones. We __strongly__ recommend that you test each of the components on your own to debug.\n",
    "\n",
    "__Hint:__ Make sure to fully simplify the gradients in your implementation!\n",
    "\n",
    "If using `np.sum` or `np.mean`, the optional `keepdims` argument (see [documentation](https://numpy.org/doc/stable/reference/generated/numpy.sum.html)) preserves the dimension of the output matrix."
   ],
   "metadata": {
    "id": "BB8SzU7pf6K4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# returns an array of the same shape as z for the gradient of sigmoid(z)\n",
    "def d_sigmoid(z):\n",
    "    pass\n",
    "\n",
    "\n",
    "# returns a (d,n) array for the gradient of nll_loss(X, y, th, th0) with respect to th for each data point\n",
    "\n",
    "\n",
    "def d_nll_loss_th(X, y, th, th0):\n",
    "    pass\n",
    "\n",
    "\n",
    "# returns a (1,n) array for the gradient of nll_loss(X, y, th, th0) with respect to th0\n",
    "\n",
    "\n",
    "def d_nll_loss_th0(X, y, th, th0):\n",
    "    pass\n",
    "\n",
    "\n",
    "# returns a (d,1) array for the gradient of llc_obj(X, y, th, th0) with respect to th\n",
    "\n",
    "\n",
    "def d_llc_obj_th(X, y, th, th0, lam):\n",
    "    pass\n",
    "\n",
    "\n",
    "# returns a (1,1) array for the gradient of llc_obj(X, y, th, th0) with respect to th0\n",
    "\n",
    "\n",
    "def d_llc_obj_th0(X, y, th, th0, lam):\n",
    "    pass\n",
    "\n",
    "\n",
    "# returns a (d+1, 1) array for the full gradient as a single vector (which includes both th, th0)\n",
    "\n",
    "\n",
    "def llc_obj_grad(X, y, th, th0, lam):\n",
    "    pass"
   ],
   "metadata": {
    "id": "Ob4WX5VTf8aG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Test dataset\n",
    "X1 = np.array([[1, 2, 3, 9, 10]])\n",
    "y1 = np.array([[1, 1, 1, 0, 0]])\n",
    "th1, th10 = np.array([[-0.31202807]]), np.array([[1.834]])\n",
    "X2 = np.array([[2, 3, 9, 12], [5, 2, 6, 5]])\n",
    "y2 = np.array([[1, 0, 1, 0]])\n",
    "th2, th20 = np.array([[-3.0, 15.0]]).T, np.array([[2.0]])\n",
    "\n",
    "# d_sigmoid Tests\n",
    "print(\"----- d_sigmoid test -----\")\n",
    "d_sig1 = d_sigmoid(np.array([[71]])).tolist()\n",
    "print(\"Test case 1:\")\n",
    "print(\"Expected:    [[0.0]] \\nYour answer:\", d_sig1)\n",
    "\n",
    "d_sig2 = d_sigmoid(np.array([[-23.0]])).tolist()\n",
    "print(\"Test case 2:\")\n",
    "print(\"Expected:    [[1.0261879629595766e-10]] \\nYour answer:\", d_sig2)\n",
    "\n",
    "d_sig3 = d_sigmoid(np.array([[71, -23.0]])).tolist()\n",
    "print(\"Test case 3:\")\n",
    "print(\"Expected:    [[0.0, 1.0261879629595766e-10]] \\nYour answer:\", d_sig3)\n",
    "\n",
    "# d_nll_loss_th Tests\n",
    "print(\"----- d_n_ll_loss_th test -----\")\n",
    "d_nll_th1 = d_nll_loss_th(X2[:, 0:1], y2[:, 0:1], th2, th20).tolist()\n",
    "print(\"Test case 4:\")\n",
    "print(\"Expected:    [[0.0], [0.0]] \\nYour answer:\", d_nll_th1)\n",
    "\n",
    "d_nll_th2 = d_nll_loss_th(X2, y2, th2, th20).tolist()\n",
    "print(\"Test case 5:\")\n",
    "print(\n",
    "    \"Expected:    [[0.0, 2.9999999996921436, 0.0, 12.0], [0.0, 1.9999999997947624, 0.0, 5.0]] \\nYour answer:\",\n",
    "    d_nll_th2,\n",
    ")\n",
    "\n",
    "# d_nll_loss_th0 Tests\n",
    "print(\"----- d_n_ll_loss_th0 test -----\")\n",
    "d_nll_th01 = d_nll_loss_th0(X2[:, 0:1], y2[:, 0:1], th2, th20).tolist()\n",
    "print(\"Test case 6:\")\n",
    "print(\"Expected:    [[0.0]] \\nYour answer:\", d_nll_th01)\n",
    "\n",
    "d_nll_th02 = d_nll_loss_th0(X2, y2, th2, th20).tolist()\n",
    "print(\"Test case 7:\")\n",
    "print(\"Expected:    [[0.0, 0.9999999998973812, 0.0, 1.0]] \\nYour answer:\", d_nll_th02)\n",
    "\n",
    "# d_llc_obj_th Tests\n",
    "print(\"----- d_llc_obj_th test -----\")\n",
    "d_llc_th1 = d_llc_obj_th(X2[:, 0:1], y2[:, 0:1], th2, th20, 0.01).tolist()\n",
    "print(\"Test case 8:\")\n",
    "print(\"Expected:    [[-0.06], [0.3]] \\nYour answer:\", d_llc_th1)\n",
    "\n",
    "d_llc_th2 = d_llc_obj_th(X2, y2, th2, th20, 0.01).tolist()\n",
    "print(\"Test case 9:\")\n",
    "print(\n",
    "    \"Expected:    [[3.6899999999230357], [2.0499999999486906]] \\nYour answer:\",\n",
    "    d_llc_th2,\n",
    ")\n",
    "\n",
    "# d_llc_obj_th0 Tests\n",
    "print(\"----- d_llc_obj_th0 test -----\")\n",
    "d_llc_th01 = d_llc_obj_th0(X2[:, 0:1], y2[:, 0:1], th2, th20, 0.01).tolist()\n",
    "print(\"Test case 10:\")\n",
    "print(\"Expected:    [[0.0]] \\nYour answer:\", d_llc_th01)\n",
    "\n",
    "d_llc_th02 = d_llc_obj_th0(X2, y2, th2, th20, 0.01).tolist()\n",
    "print(\"Test case 11:\")\n",
    "print(\"Expected:    [[0.4999999999743453]] \\nYour answer:\", d_llc_th02)\n",
    "\n",
    "# llc_obj_grad Tests\n",
    "print(\"----- llc_obj_grad test -----\")\n",
    "llc_grad1 = llc_obj_grad(X2, y2, th2, th20, 0.01).tolist()\n",
    "print(\"Test case 12:\")\n",
    "print(\n",
    "    \"Expected:    [[3.6899999999230357], [2.0499999999486906], [0.4999999999743453]] \\nYour answer:\",\n",
    "    llc_grad1,\n",
    ")\n",
    "\n",
    "llc_grad2 = llc_obj_grad(X2[:, 0:1], y2[:, 0:1], th2, th20, 0.01).tolist()\n",
    "print(\"Test case 13:\")\n",
    "print(\"Expected:    [[-0.06], [0.3], [0.0]] \\nYour answer:\", llc_grad2)"
   ],
   "metadata": {
    "id": "h6aZzFDGf-FJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.3) LLC minimize\n",
    "\n",
    "Putting it all together, use the functions you built earlier to write a gradient descent minimizer for the LLC objective. You do not need to paste in your previous definitions; you can just call the ones you've defined above. You will need to call gd the gradient descent function which you implemented in HW 3; your function `llc_min` should return the values that gd does. We have provided you with a sequence of step sizes already below.\n",
    "\n",
    "- Initialize all the separator parameters $\\theta$, $\\theta_0$ to 0,\n",
    "- use the step size function provided below, and\n",
    "- specify 10 iterations.\n",
    "\n",
    "__Hint:__ the `f` that we feed into `gd` can only have a single column vector as its parameter; however, to call the objective function you've written, you need both `theta` and `theta_0`. Think of a way to pass both of them into `f` and then unpack them to call the objective function. Look back at the structure of what `llc_obj_grad` returns in the previous problem."
   ],
   "metadata": {
    "id": "_fDsvcFwf__C"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def llc_min(data, labels, lam):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        data: dxn\n",
    "        labels: 1xn\n",
    "        lam: scalar\n",
    "    Returns:\n",
    "        same output as gd\n",
    "    \"\"\"\n",
    "\n",
    "    def llc_min_step_size_fn(i):\n",
    "        return 2 / (i + 1) ** 0.5\n",
    "\n",
    "    pass"
   ],
   "metadata": {
    "id": "FfAptDFygBnt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some test cases are shown below, where an additional separable test dataset has been specified.\n",
    "```\n",
    "def separable_medium():\n",
    "    X = np.array([[2, -1, 1, 1],\n",
    "                  [-2, 2, 2, -1]])\n",
    "    y = np.array([[1, 0, 1, 0]])\n",
    "    return X, y\n",
    "sep_m_separator = np.array([[ 2.69231855], [ 0.67624906]]), np.array([[-3.02402521]])\n",
    "```"
   ],
   "metadata": {
    "id": "LnBjBVRugDmS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def package_ans(gd_vals):\n",
    "    x, fx = gd_vals\n",
    "    return [x.tolist(), fx.tolist()]"
   ],
   "metadata": {
    "id": "NxZPPt7vgEDD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "x_1, y_1 = super_simple_separable()\n",
    "llc_min1 = package_ans(llc_min(x_1, y_1, 0.0001))\n",
    "print(\"----- llc_min test on super_simple_separable dataset -----\")\n",
    "print(\n",
    "    \"Expected:    [[[-1.897383932746237], [3.3962308285238088], [-0.3185808665118801]], 0.30406118305471325] \\nYour answer:\",\n",
    "    llc_min1,\n",
    ")\n",
    "\n",
    "x_1, y_1 = separable_medium()\n",
    "llc_min2 = package_ans(llc_min(x_1, y_1, 0.0001))\n",
    "print(\"----- llc_min test on separable_medium dataset -----\")\n",
    "print(\n",
    "    \"Expected:    [[[1.377362757219987], [0.3661947799990386], [-0.7768724769832256]], 0.380271389072302] \\nYour answer:\",\n",
    "    llc_min2,\n",
    ")"
   ],
   "metadata": {
    "id": "zMDmnIWPgFpQ"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
